# Import necessary libraries
import requests
from bs4 import BeautifulSoup
import json

# Function to search Wikipedia using the Wikipedia API
def search_wikipedia(query):
    """
    This function uses the Wikipedia API to search for articles related to the query.
    It returns a list of articles (limited to one result).

    Parameters:
    query (str): The search term to query Wikipedia.

    Returns:
    list: A list of search results, each containing the article title.
    """
    # Define the parameters for the API request
    params = {
        'action': 'query',  # Define the action as 'query'
        'list': 'search',  # Specify the type of query (searching for articles)
        'srsearch': query,  # The search term we want to find
        'format': 'json',  # Return the results in JSON format
        'srlimit': 1  # Limit the results to 1 article (optional)
    }

    # Make the request to the Wikipedia API
    response = requests.get("https://en.wikipedia.org/w/api.php", params=params)
    
    # Return the search results if the response status is 200 (OK), else return an empty list
    return response.json().get('query', {}).get('search', []) if response.status_code == 200 else []

# Function to fetch the content of a Wikipedia article
def fetch_article_content(title):
    """
    This function retrieves the content of a Wikipedia article by its title.
    It extracts and returns the text of the article (only the paragraph text).

    Parameters:
    title (str): The title of the Wikipedia article.

    Returns:
    str: The article content (text of paragraphs) or None if not found.
    """
    # Construct the URL of the Wikipedia article
    url = f"https://en.wikipedia.org/wiki/{title.replace(' ', '_')}"
    
    # Request the article page and parse the content with BeautifulSoup
    soup = BeautifulSoup(requests.get(url).content, 'html.parser')
    
    # Find the main content section (with class 'mw-parser-output')
    content = soup.find('div', class_='mw-parser-output')
    
    # Extract and return the text of all paragraphs within the content
    return ' '.join([p.text for p in content.find_all('p') if p.text.strip()]) if content else None

# Main function to fetch and save articles to a JSON file
def fetch_and_save_articles(queries, filename="wikipedia_articles.json"):
    """
    This function combines the search and fetch functions to search for articles,
    retrieve their content, and save the data into a JSON file.

    Parameters:
    queries (list): A list of search queries (strings).
    filename (str): The name of the file where the articles will be saved (default is 'wikipedia_articles.json').

    Returns:
    None
    """
    articles = []  # List to store the fetched articles

    # Loop through each query in the list of search terms
    for query in queries:
        # Perform a Wikipedia search for the query
        search_results = search_wikipedia(query)
        
        # Loop through the search results (only one result is fetched in this case)
        for result in search_results:
            # Fetch the content of the article based on the title from the search result
            content = fetch_article_content(result['title'])
            
            # Print the title of the article being processed
            print(result['title'])
            
            # If content was successfully fetched, append it to the articles list
            if content:
                articles.append({'title': result['title'], 'content': content})
    
    # If articles were found, save them to a JSON file
    if articles:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(articles, f, ensure_ascii=False, indent=4)
        # Print the number of articles saved
        print(f"Saved {len(articles)} articles to {filename}.")
    else:
        # If no articles were found, print a message
        print("No articles found.")
