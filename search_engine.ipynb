{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search from Wikipedia\n",
    "This section performs a search on Wikipedia for a given query and fetches the article content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# Search from wikipedia\n",
    "def search_wikipedia(query):\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'list': 'search',\n",
    "        'srsearch': query,\n",
    "        'format': 'json',\n",
    "        'srlimit': 1\n",
    "    }\n",
    "    response = requests.get(\"https://en.wikipedia.org/w/api.php\", params=params)\n",
    "    return response.json().get('query', {}).get('search', []) if response.status_code == 200 else []\n",
    "\n",
    "# Fetch article's content\n",
    "def fetch_article_content(title):\n",
    "    url = f\"https://en.wikipedia.org/wiki/{title.replace(' ', '_')}\"\n",
    "    soup = BeautifulSoup(requests.get(url).content, 'html.parser')\n",
    "    content = soup.find('div', class_='mw-parser-output')\n",
    "    return ' '.join([p.text for p in content.find_all('p') if p.text.strip()]) if content else None\n",
    "\n",
    "# Main function to fetch articles and save them to a json\n",
    "def fetch_and_save_articles(queries, filename=\"wikipedia_articles.json\"):\n",
    "    articles = []\n",
    "    for query in queries:\n",
    "        search_results = search_wikipedia(query)\n",
    "        for result in search_results:\n",
    "            content = fetch_article_content(result['title'])\n",
    "            print(result['title'])\n",
    "            if content:\n",
    "                articles.append({'title': result['title'], 'content': content})\n",
    "    if articles:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(articles, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Saved {len(articles)} articles to {filename}.\")\n",
    "    else:\n",
    "        print(\"No articles found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:\n",
    "Deep learning
Document classification
Information retrieval
Natural language processing
Learning to rank
Information retrieval
Recommender system
Large language model
Prompt engineering
Machine learning
Recurrent neural network
Search algorithm
Climate change
History of art
Nutrition and Health
Olympic Games
Caving
Saved 15 articles to wikipedia_articles.json."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Saving Articles\n",
    "This section preprocesses the articles by removing stop-words and saves them in a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "STOP_WORDS = {\n",
    "    \"a\", \"an\", \"and\", \"the\", \"is\", \"in\", \"it\", \"of\", \"to\", \"with\", \"on\", \"for\", \"this\", \"that\", \"at\", \"by\", \"from\",\n",
    "    \"as\", \"are\", \"be\", \"or\", \"but\", \"not\", \"was\", \"which\", \"so\", \"if\", \"can\", \"will\", \"would\", \"has\", \"have\", \"had\"\n",
    "}\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocesses text by removing special characters, converting to lowercase, and remove stop-words.\"\"\"\n",
    "    return ' '.join(\n",
    "        token for token in re.sub(r'[^a-zA-Z\s]', '', text).lower().split() \n",
    "        if token not in STOP_WORDS\n",
    "    )\n",
    "\n",
    "def read_json_file(filename):\n",
    "    \"\"\"Reads JSON file and returns its content or None on failure.\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_articles(input_file, output_file):\n",
    "    \"\"\"Processes articles from input JSON file and saves preprocessed data to output JSON file.\"\"\"\n",
    "    if articles := read_json_file(input_file):\n",
    "        processed_articles = [\n",
    "            {'title': article.get('title', 'No Title'), \n",
    "             'content': preprocess_text(article.get('content', ''))}\n",
    "            for article in articles\n",
    "        ]\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(processed_articles, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Processed articles saved in '{output_file}'.\")\n",
    "    else:\n",
    "        print(\"No articles to process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:\n",
    "Unprocessed:These architectures have been applied to fields including computer vision, speech recognition,
natural language processing, machine translation, bioinformatics, drug design, medical image analysis,
climate science, material inspection and board game programs, where they have produced results comparable 
to and in some cases surpassing human expert performance.[3][4][5]
Processed: these architectures been applied fields including computer vision speech recognition natural
language processing machine translation bioinformatics drug design medical image analysis climate science
material inspection board game programs where they produced results comparable some cases surpassing human expert performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Inverted Index\n",
    "This section creates an inverted index from the processed articles and saves it in a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Loads preprocessed articles from a JSON file\n",
    "def load_processed_articles(filename):\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            articles = json.load(file)\n",
    "            print(f\"Loaded {len(articles)} articles from '{filename}'.\")\n",
    "            return articles\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file: {e}\")\n",
    "        return []\n",
    "\n",
    "# Creates an inverted index from a list of articles\n",
    "def create_inverted_index(articles):\n",
    "    inverted_index = defaultdict(list)\n",
    "    for article_id, article in enumerate(articles):\n",
    "        for word in article.get('content', '').split():\n",
    "            if article_id not in inverted_index[word]:\n",
    "                inverted_index[word].append(article_id)\n",
    "    return inverted_index\n",
    "\n",
    "# Saves the inverted index to a JSON file\n",
    "def save_inverted_index(inverted_index, filename=\"inverted_index.json\"):\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as file:\n",
    "            json.dump(inverted_index, file, ensure_ascii=False, indent=4)\n",
    "        print(f\"Inverted index saved in '{filename}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:\n",
    "    'deep': [
        0,
        3,
        4,
        6,
        7,
        9,
        10,
        12
    ],
    'learning': [
        0,
        3,
        4,
        6,
        7,
        8,
        9,
        10
    ],
    'subset': [
        0,
        9
    ],
    'machine': [
        0,
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9,
        10
    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Algorithms Implementation\n",
    "This section implements different search algorithms: Boolean Search, TF-IDF Search, and BM25 Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Placeholder for Boolean search, TF-IDF search, and BM25 search functions\n",
    "# These are placeholders to represent the structure for implementing different search algorithms.\n",
    "\n",
    "def boolean_search(query, inverted_index, articles):\n",
    "    # Basic Boolean search that returns articles containing all the query terms.\n",
    "    result = set(range(len(articles)))\n",
    "    for term in query.split():\n",
    "        if term in inverted_index:\n",
    "            result &= set(inverted_index[term])\n",
    "        else:\n",
    "            result = set()  # If any term is not found, no result is returned.\n",
    "    return list(result)\n",
    "\n",
    "def tfidf_search(query, articles):\n",
    "    # TF-IDF search using scikit-learn's TfidfVectorizer\n",
    "    corpus = [article['content'] for article in articles]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    cosine_similarities = np.dot(tfidf_matrix, query_vector.T).toarray()\n",
    "    ranked_articles = cosine_similarities.flatten().argsort()[::-1]\n",
    "    return ranked_articles\n",
    "\n",
    "def bm25_search(query, inverted_index, articles):\n",
    "    # BM25 search using the rank_bm25 package\n",
    "    corpus = [article['content'].split() for article in articles]\n",
    "    bm25 = BM25Okapi(corpus)\n",
    "    query_terms = query.split()\n",
    "    scores = bm25.get_scores(query_terms)\n",
    "    ranked_articles = np.argsort(scores)[::-1]\n",
    "    return ranked_articles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:\n",
    "Choose algorithm (1: Boolean, 2: TF-IDF, 3: BM25, 'exit' to quit): 1
Enter your query: caving

Found 1 result(s):
- Caving

Choose algorithm (1: Boolean, 2: TF-IDF, 3: BM25, 'exit' to quit): 2
Enter your query: caving

Found 1 result(s):
- Caving

Choose algorithm (1: Boolean, 2: TF-IDF, 3: BM25, 'exit' to quit): 3
Enter your query: caving

Found 1 result(s):
- Caving"
   ]
  }
 ]
}
