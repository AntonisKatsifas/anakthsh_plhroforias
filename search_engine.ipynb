{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia Web crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section performs a search on Wikipedia for a given query and fetches the article content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep learning\n",
      "Document classification\n",
      "Information retrieval\n",
      "Natural language processing\n",
      "Learning to rank\n",
      "Information retrieval\n",
      "Recommender system\n",
      "Large language model\n",
      "Prompt engineering\n",
      "Machine learning\n",
      "Recurrent neural network\n",
      "Search algorithm\n",
      "Climate change\n",
      "History of art\n",
      "Nutrition and Health\n",
      "Olympic Games\n",
      "Caving\n",
      "Saved 15 articles to wikipedia_articles.json.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "\n",
    "# Search from wikipedia\n",
    "def search_wikipedia(query):\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'list': 'search',\n",
    "        'srsearch': query,\n",
    "        'format': 'json',\n",
    "        'srlimit': 1\n",
    "    }\n",
    "    response = requests.get(\"https://en.wikipedia.org/w/api.php\", params=params)\n",
    "    return response.json().get('query', {}).get('search', []) if response.status_code == 200 else []\n",
    "\n",
    "# Fetch article's content\n",
    "def fetch_article_content(title):\n",
    "    url = f\"https://en.wikipedia.org/wiki/{title.replace(' ', '_')}\"\n",
    "    soup = BeautifulSoup(requests.get(url).content, 'html.parser')\n",
    "    content = soup.find('div', class_='mw-parser-output')\n",
    "    return ' '.join([p.text for p in content.find_all('p') if p.text.strip()]) if content else None\n",
    "\n",
    "# Main function to fetch articles and save them to a json\n",
    "def fetch_and_save_articles(queries, filename=\"wikipedia_articles.json\"):\n",
    "    articles = []\n",
    "    for query in queries:\n",
    "        search_results = search_wikipedia(query)\n",
    "        for result in search_results:\n",
    "            content = fetch_article_content(result['title'])\n",
    "            print(result['title'])\n",
    "            if content:\n",
    "                articles.append({'title': result['title'], 'content': content})\n",
    "    if articles:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(articles, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Saved {len(articles)} articles to {filename}.\")\n",
    "    else:\n",
    "        print(\"No articles found.\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    queries = \"Deep learning,Document classification,Information retrieval,Natural language processing,Learning to rank,Information retrieval,Recommender system,Large language model,Prompt engineering,Machine learning,Recurrent neural network,Ranking Algorithms,Climate Change,History of Art,Health and Nutrition,The History of the Olympic Games,Cave exploration\".split(\",\")\n",
    "    fetch_and_save_articles(queries)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section preprocesses the articles by removing stop-words and saves them in a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed articles saved in 'processed_wikipedia_articles.json'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "STOP_WORDS = {\n",
    "    \"a\", \"an\", \"and\", \"the\", \"is\", \"in\", \"it\", \"of\", \"to\", \"with\", \"on\", \"for\", \"this\", \"that\", \"at\", \"by\", \"from\",\n",
    "    \"as\", \"are\", \"be\", \"or\", \"but\", \"not\", \"was\", \"which\", \"so\", \"if\", \"can\", \"will\", \"would\", \"has\", \"have\", \"had\"\n",
    "}\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocesses text by removing special characters, converting to lowercase, and remove stop-words.\"\"\"\n",
    "    return ' '.join(\n",
    "        token for token in re.sub(r'[^a-zA-Z\\s]', '', text).lower().split() \n",
    "        if token not in STOP_WORDS\n",
    "    )\n",
    "\n",
    "def read_json_file(filename):\n",
    "    \"\"\"Reads JSON file and returns its content or None on failure.\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_articles(input_file, output_file):\n",
    "    \"\"\"Processes articles from input JSON file and saves preprocessed data to output JSON file.\"\"\"\n",
    "    if articles := read_json_file(input_file):\n",
    "        processed_articles = [\n",
    "            {'title': article.get('title', 'No Title'), \n",
    "             'content': preprocess_text(article.get('content', ''))}\n",
    "            for article in articles\n",
    "        ]\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(processed_articles, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Processed articles saved in '{output_file}'.\")\n",
    "    else:\n",
    "        print(\"No articles to process.\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    process_articles(\"wikipedia_articles.json\", \"processed_wikipedia_articles.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverted index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section creates an inverted index from the processed articles and saves it in a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15 articles from 'processed_wikipedia_articles.json'.\n",
      "Inverted index saved in 'inverted_index.json'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Loads preprocessed articles from a JSON file\n",
    "def load_processed_articles(filename):\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            articles = json.load(file)\n",
    "            print(f\"Loaded {len(articles)} articles from '{filename}'.\")\n",
    "            return articles\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file: {e}\")\n",
    "        return []\n",
    "\n",
    "# Creates an inverted index from a list of articles\n",
    "def create_inverted_index(articles):\n",
    "    inverted_index = defaultdict(list)\n",
    "    for article_id, article in enumerate(articles):\n",
    "        for word in article.get('content', '').split():\n",
    "            if article_id not in inverted_index[word]:\n",
    "                inverted_index[word].append(article_id)\n",
    "    return inverted_index\n",
    "\n",
    "# Saves the inverted index to a JSON file\n",
    "def save_inverted_index(inverted_index, filename=\"inverted_index.json\"):\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as file:\n",
    "            json.dump(inverted_index, file, ensure_ascii=False, indent=4)\n",
    "        print(f\"Inverted index saved in '{filename}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving file: {e}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    input_filename = \"processed_wikipedia_articles.json\"\n",
    "    output_filename = \"inverted_index.json\"\n",
    "    articles = load_processed_articles(input_filename)\n",
    "    save_inverted_index(create_inverted_index(articles), output_filename)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section implements different search algorithms: Boolean Search, TF-IDF Search, and BM25 Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 6 result(s):\n",
      "- Deep learning\n",
      "- Large language model\n",
      "- Prompt engineering\n",
      "- Machine learning\n",
      "- Recurrent neural network\n",
      "- History of art\n",
      "\n",
      "Found 1 result(s):\n",
      "- Caving\n",
      "\n",
      "Found 7 result(s):\n",
      "- Deep learning\n",
      "- Natural language processing\n",
      "- Recommender system\n",
      "- Large language model\n",
      "- Prompt engineering\n",
      "- Machine learning\n",
      "- Search algorithm\n",
      "Exiting search engine. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "def load_json(filename):\n",
    "    \"\"\"Load a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            return json.load(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading '{filename}': {e}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_query(query):\n",
    "    \"\"\"Clean and tokenize the query.\"\"\"\n",
    "    return re.sub(r'[^a-zA-Z\\s]', '', query).lower().split()\n",
    "\n",
    "def boolean_search(query_tokens, inverted_index):\n",
    "    \"\"\"Perform boolean search on the inverted index.\"\"\"\n",
    "    results, operation = set(), \"OR\"\n",
    "    found_results = False  # Flag to check if we found any results\n",
    "\n",
    "    for token in query_tokens:\n",
    "        if token.upper() in [\"AND\", \"OR\", \"NOT\"]:\n",
    "            operation = token.upper()\n",
    "        else:\n",
    "            postings = set(inverted_index.get(token, []))\n",
    "            if not postings:  # If no postings found for this token, skip it\n",
    "                continue\n",
    "\n",
    "            if operation == \"OR\":\n",
    "                results |= postings\n",
    "                found_results = True\n",
    "            elif operation == \"AND\":\n",
    "                if not results:  # If no previous results, initialize with the first set\n",
    "                    results = postings\n",
    "                else:\n",
    "                    results &= postings\n",
    "            elif operation == \"NOT\":\n",
    "                results -= postings\n",
    "\n",
    "    # If no results are found, return an empty set\n",
    "    return results if found_results else set()\n",
    "\n",
    "\n",
    "def tfidf_search(query_tokens, articles, threshold=0.1):\n",
    "    \"\"\"Perform TF-IDF search on the articles.\"\"\"\n",
    "    corpus = [article['content'] for article in articles]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    query_vector = vectorizer.transform([' '.join(query_tokens)])\n",
    "    scores = np.dot(query_vector, tfidf_matrix.T).toarray().flatten()\n",
    "\n",
    "    # Only return results with scores above the threshold\n",
    "    return [i for i, score in enumerate(scores) if score > threshold]\n",
    "\n",
    "def bm25_search(query_tokens, articles, threshold=0.1):\n",
    "    \"\"\"Perform BM25 search on the articles.\"\"\"\n",
    "    corpus = [article['content'].split() for article in articles]\n",
    "    bm25 = BM25Okapi(corpus)\n",
    "    scores = bm25.get_scores(query_tokens)\n",
    "\n",
    "    # Only return results with scores above the threshold\n",
    "    return [i for i, score in enumerate(scores) if score > threshold]\n",
    "\n",
    "\n",
    "def search_query(query, inverted_index, articles, algorithm):\n",
    "    \"\"\"Search the query using the selected algorithm.\"\"\"\n",
    "    query_tokens = preprocess_query(query)\n",
    "    if algorithm == \"boolean\":\n",
    "        matched_ids = boolean_search(query_tokens, inverted_index)\n",
    "        return [articles[doc_id] for doc_id in matched_ids] if matched_ids else []\n",
    "    elif algorithm == \"tfidf\":\n",
    "        matched_ids = tfidf_search(query_tokens, articles)\n",
    "        return [articles[doc_id] for doc_id in matched_ids] if matched_ids else []\n",
    "    elif algorithm == \"bm25\":\n",
    "        matched_ids = bm25_search(query_tokens, articles)\n",
    "        return [articles[doc_id] for doc_id in matched_ids] if matched_ids else []\n",
    "    return []\n",
    "\n",
    "def search_interface():\n",
    "    \"\"\"Search interface to interact with the user.\"\"\"\n",
    "    inverted_index = load_json(\"inverted_index.json\")\n",
    "    articles = load_json(\"processed_wikipedia_articles.json\")\n",
    "    if not inverted_index or not articles:\n",
    "        print(\"Error loading data. Exiting...\")\n",
    "        return\n",
    "\n",
    "    algorithms = {\"1\": \"boolean\", \"2\": \"tfidf\", \"3\": \"bm25\"}\n",
    "    while True:\n",
    "        choice = input(\"\\nChoose algorithm (1: Boolean, 2: TF-IDF, 3: BM25, 'exit' to quit): \")\n",
    "        if choice.lower() == \"exit\":\n",
    "            print(\"Exiting search engine. Goodbye!\")\n",
    "            break\n",
    "        algorithm = algorithms.get(choice)\n",
    "        if not algorithm:\n",
    "            print(\"Invalid choice. Try again.\")\n",
    "            continue\n",
    "\n",
    "        query = input(\"Enter your query: \")\n",
    "        results = search_query(query, inverted_index, articles, algorithm)\n",
    "        if results:\n",
    "            print(f\"\\nFound {len(results)} result(s):\")\n",
    "            for result in results:\n",
    "                print(f\"- {result['title']}\")\n",
    "        else:\n",
    "            print(\"No results found.\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    search_interface()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section evaluates the effectiveness of different search algorithms: Boolean Search, TF-IDF Search, and BM25 Search. Each algorithm is evaluated using precision, recall, F1-Score, and mean average precision (MAP) based on test queries and relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating using boolean search...\n",
      "Query: 'What is machine learning?' - Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000, MAP: 1.0000\n",
      "Query: 'Techniques for recommending items' - Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000, MAP: 1.0000\n",
      "Query: 'How to process natural language?' - Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000, MAP: 1.0000\n",
      "Query: 'Algorithms for ranking results' - Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000, MAP: 1.0000\n",
      "Query: 'What are large language models used for?' - Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000, MAP: 1.0000\n",
      "Query: 'Deep learning in AI systems' - Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000, MAP: 1.0000\n",
      "Query: 'How to design search algorithms?' - Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000, MAP: 1.0000\n",
      "Query: 'Applications of recurrent neural networks' - Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000, MAP: 1.0000\n",
      "Query: 'Methods for improving information retrieval' - Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000, MAP: 1.0000\n",
      "Query: 'How to engineer effective prompts?' - Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000, MAP: 1.0000\n",
      "\n",
      "Average Precision: 1.0000\n",
      "Average Recall: 1.0000\n",
      "Average F1-Score: 1.0000\n",
      "Average MAP: 1.0000\n",
      "\n",
      "Evaluating using tfidf search...\n",
      "Query: 'What is machine learning?' - Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000, MAP: 1.0000\n",
      "Query: 'Techniques for recommending items' - Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000, MAP: 1.0000\n",
      "Query: 'How to process natural language?' - Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000, MAP: 1.0000\n",
      "Query: 'Algorithms for ranking results' - Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000, MAP: 1.0000\n",
      "Query: 'What are large language models used for?' - Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000, MAP: 1.0000\n",
      "Query: 'Deep learning in AI systems' - Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000, MAP: 1.0000\n",
      "Query: 'How to design search algorithms?' - Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000, MAP: 1.0000\n",
      "Query: 'Applications of recurrent neural networks' - Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000, MAP: 1.0000\n",
      "Query: 'Methods for improving information retrieval' - Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000, MAP: 1.0000\n",
      "Query: 'How to engineer effective prompts?' - Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000, MAP: 1.0000\n",
      "\n",
      "Average Precision: 1.0000\n",
      "Average Recall: 1.0000\n",
      "Average F1-Score: 1.0000\n",
      "Average MAP: 1.0000\n",
      "\n",
      "Evaluating using bm25 search...\n",
      "Query: 'What is machine learning?' - Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000, MAP: 1.0000\n",
      "Query: 'Techniques for recommending items' - Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000, MAP: 1.0000\n",
      "Query: 'How to process natural language?' - Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000, MAP: 1.0000\n",
      "Query: 'Algorithms for ranking results' - Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000, MAP: 1.0000\n",
      "Query: 'What are large language models used for?' - Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000, MAP: 1.0000\n",
      "Query: 'Deep learning in AI systems' - Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000, MAP: 1.0000\n",
      "Query: 'How to design search algorithms?' - Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000, MAP: 1.0000\n",
      "Query: 'Applications of recurrent neural networks' - Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000, MAP: 1.0000\n",
      "Query: 'Methods for improving information retrieval' - Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000, MAP: 1.0000\n",
      "Query: 'How to engineer effective prompts?' - Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000, MAP: 1.0000\n",
      "\n",
      "Average Precision: 1.0000\n",
      "Average Recall: 1.0000\n",
      "Average F1-Score: 1.0000\n",
      "Average MAP: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load inverted index\n",
    "def load_inverted_index(filename=\"inverted_index.json\"):\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            return json.load(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading inverted index: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load articles\n",
    "def load_articles(filename=\"processed_wikipedia_articles.json\"):\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            articles = json.load(file)\n",
    "            return articles\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading articles: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Query processing\n",
    "def preprocess_query(query):\n",
    "    query = re.sub(r'[^a-zA-Z\\s]', '', query)  \n",
    "    query = query.lower()  \n",
    "    tokens = query.split() \n",
    "    return tokens\n",
    "\n",
    "# Boolean search\n",
    "def boolean_search(query_tokens, inverted_index):\n",
    "    results = set()\n",
    "    operation = \"OR\"  # if there are not logical operators OR is used\n",
    "\n",
    "    for token in query_tokens:\n",
    "        if token.upper() in [\"AND\", \"OR\", \"NOT\"]:\n",
    "            operation = token.upper()\n",
    "        else:\n",
    "            postings = set(inverted_index.get(token, []))\n",
    "            \n",
    "            if operation == \"OR\":\n",
    "                results |= postings\n",
    "            elif operation == \"AND\":\n",
    "                results &= postings\n",
    "            elif operation == \"NOT\":\n",
    "                results -= postings\n",
    "\n",
    "    return results\n",
    "\n",
    "#  TF-IDF search\n",
    "def tfidf_search(query_tokens, articles):\n",
    "\n",
    "    # Get all document content\n",
    "    documents = [article['content'] for article in articles]\n",
    "    \n",
    "    # Create vectorizer TF-IDF\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(documents)\n",
    "    \n",
    "    # Create query vector\n",
    "    query = ' '.join(query_tokens)\n",
    "    query_vector = vectorizer.transform([query])\n",
    "\n",
    "    # cosine similarity\n",
    "    cosine_similarities = np.array(X.dot(query_vector.T).toarray()).flatten()\n",
    "    \n",
    "    return cosine_similarities\n",
    "\n",
    "def bm25_search(query_tokens, articles):\n",
    "    # Tokenize the articles' text\n",
    "    tokenized_articles = [word_tokenize(article['content'].lower()) for article in articles]\n",
    "    \n",
    "    # Initialize the BM25 model\n",
    "    bm25 = BM25Okapi(tokenized_articles)\n",
    "    \n",
    "    # Score the query against all documents\n",
    "    scores = bm25.get_scores(query_tokens)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_search_algorithm(query, relevant_docs, inverted_index, articles, algorithm):\n",
    "\n",
    "    query_tokens = preprocess_query(query)\n",
    "    \n",
    "    if algorithm == \"boolean\":\n",
    "        retrieved_docs = boolean_search(query_tokens, inverted_index)\n",
    "    elif algorithm == \"tfidf\":\n",
    "        cosine_similarities = tfidf_search(query_tokens, articles)\n",
    "        ranked_docs = sorted(enumerate(cosine_similarities), key=lambda x: x[1], reverse=True)\n",
    "        retrieved_docs = [doc_id for doc_id, _ in ranked_docs]\n",
    "    elif algorithm == \"bm25\":\n",
    "        scores = bm25_search(query_tokens, articles)\n",
    "        ranked_docs = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "        retrieved_docs = [doc_id for doc_id, _ in ranked_docs]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    # Υπολογισμός ακρίβειας, ανάκλησης και F1-score\n",
    "    # if there are not related articles 0 the variables\n",
    "    y_true = [1 if doc_id in relevant_docs else 0 for doc_id in retrieved_docs]\n",
    "    y_pred = [1 if doc_id in relevant_docs else 0 for doc_id in retrieved_docs]\n",
    "\n",
    "    if sum(y_true) == 0 and sum(y_pred) == 0:\n",
    "        precision = recall = f1 = 0.0\n",
    "    else:\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    # Calculate (MAP)\n",
    "    if len(retrieved_docs) == 0:\n",
    "        ap = 0.0\n",
    "    else:\n",
    "        ap = average_precision_score(y_true, y_pred)\n",
    "\n",
    "    return precision, recall, f1, ap\n",
    "\n",
    "# Create test queries, uncomment one of them only to test cases\n",
    "def generate_test_queries():\n",
    "    # test_queries = [\n",
    "    #     {\"query\": \"machine learning\", \"relevant_docs\": [0, 9, 7]},\n",
    "    #     {\"query\": \"information retrieval\", \"relevant_docs\": [2, 5, 4]},\n",
    "    #     {\"query\": \"natural language processing\", \"relevant_docs\": [3, 7, 8]},\n",
    "    #     {\"query\": \"deep learning\", \"relevant_docs\": [0, 10, 7]},\n",
    "    #     {\"query\": \"search algorithm\", \"relevant_docs\": [11, 2]},\n",
    "    #     {\"query\": \"recommender system\", \"relevant_docs\": [6, 4]},\n",
    "    #     {\"query\": \"prompt engineering\", \"relevant_docs\": [8, 7]},\n",
    "    #     {\"query\": \"learning to rank\", \"relevant_docs\": [4, 2]},\n",
    "    # ]\n",
    "    test_queries = [\n",
    "        {\"query\": \"What is machine learning?\", \"relevant_docs\": [0, 9]},\n",
    "        {\"query\": \"Techniques for recommending items\", \"relevant_docs\": [6, 8]},\n",
    "        {\"query\": \"How to process natural language?\", \"relevant_docs\": [3, 7]},\n",
    "        {\"query\": \"Algorithms for ranking results\", \"relevant_docs\": [4, 5]},\n",
    "        {\"query\": \"What are large language models used for?\", \"relevant_docs\": [7, 8]},\n",
    "        {\"query\": \"Deep learning in AI systems\", \"relevant_docs\": [0, 1]},\n",
    "        {\"query\": \"How to design search algorithms?\", \"relevant_docs\": [11, 4]},\n",
    "        {\"query\": \"Applications of recurrent neural networks\", \"relevant_docs\": [9, 10]},\n",
    "        {\"query\": \"Methods for improving information retrieval\", \"relevant_docs\": [5, 3]},\n",
    "        {\"query\": \"How to engineer effective prompts?\", \"relevant_docs\": [8, 7]}\n",
    "    ]\n",
    "    # test_queries = [\n",
    "    #     {\"query\": \"applications of deep learning in natural language processing\", \"relevant_docs\": [0, 3, 9]},\n",
    "    #     {\"query\": \"methods for document classification and information retrieval\", \"relevant_docs\": [1, 2, 4]},\n",
    "    #     {\"query\": \"techniques in recommender systems and large language models\", \"relevant_docs\": [6, 7, 8]},\n",
    "    #     {\"query\": \"overview of machine learning and recurrent neural networks\", \"relevant_docs\": [9, 10]},\n",
    "    #     {\"query\": \"ranking algorithms in search engines\", \"relevant_docs\": [11, 2, 4]},\n",
    "    #     {\"query\": \"climate change impact on global ecosystems\", \"relevant_docs\": [12]},\n",
    "    #     {\"query\": \"renaissance art history and its influence\", \"relevant_docs\": [13]},\n",
    "    #     {\"query\": \"importance of balanced diet in health and nutrition\", \"relevant_docs\": [14]},\n",
    "    #     {\"query\": \"history and significance of the Olympic Games\", \"relevant_docs\": [15]},\n",
    "    #     {\"query\": \"exploration techniques in cave environments\", \"relevant_docs\": [16]}\n",
    "    # ]\n",
    "    # test_queries = [\n",
    "    #     {\"query\": \"machine AND learning\", \"relevant_docs\": [0, 10]},\n",
    "    #     {\"query\": \"deep learning OR artificial intelligence\", \"relevant_docs\": [2, 8, 10]},\n",
    "    #     {\"query\": \"natural language processing NOT deep learning\", \"relevant_docs\": [3, 4, 5]},\n",
    "    #     {\"query\": \"machine learning AND (deep learning OR neural networks) NOT artificial intelligence\", \"relevant_docs\": [0, 2, 8]}\n",
    "    # ]\n",
    "    # test_queries = [\n",
    "    #     {\"query\": \"deep learning AND artificial intelligence AND health and nutrition\", \"relevant_docs\": [0, 14, 16]},\n",
    "    #     {\"query\": \"machine learning AND climate change AND renaissance art\", \"relevant_docs\": [9, 12, 13]},\n",
    "    #     {\"query\": \"information retrieval AND history of art AND cave exploration\", \"relevant_docs\": [2, 13, 16]},\n",
    "    #     {\"query\": \"machine learning AND recommender systems AND neural networks\", \"relevant_docs\": [9, 6, 10]},\n",
    "    #     {\"query\": \"deep learning AND large language models AND health and nutrition\", \"relevant_docs\": [0, 7, 14]},\n",
    "    #     {\"query\": \"search algorithms AND climate change AND prompt engineering\", \"relevant_docs\": [11, 12, 8]},\n",
    "    #     {\"query\": \"natural language processing AND document classification AND deep learning AND cave exploration\", \"relevant_docs\": [3, 1, 0, 16]},\n",
    "    #     {\"query\": \"ranking algorithms AND recommender systems AND health and nutrition\", \"relevant_docs\": [11, 6, 14]},\n",
    "    #     {\"query\": \"history of art AND large language models AND artificial intelligence\", \"relevant_docs\": [13, 7, 0]},\n",
    "    #     {\"query\": \"machine learning AND deep learning NOT artificial intelligence\", \"relevant_docs\": [9, 0]},\n",
    "    #     {\"query\": \"natural language processing AND recommender systems NOT machine learning\", \"relevant_docs\": [3, 6]},\n",
    "    #     {\"query\": \"climate change AND health and nutrition NOT deep learning\", \"relevant_docs\": [12, 14]},\n",
    "    #     {\"query\": \"history of art AND health and nutrition AND artificial intelligence\", \"relevant_docs\": [13, 14, 0]},\n",
    "    #     {\"query\": \"climate change AND cave exploration AND prompt engineering\", \"relevant_docs\": [12, 16, 8]},\n",
    "    #     {\"query\": \"renaissance art AND machine learning AND recommender systems\", \"relevant_docs\": [13, 9, 6]},\n",
    "    #     {\"query\": \"recommender systems\", \"relevant_docs\": [6]},\n",
    "    #     {\"query\": \"ranking algorithms\", \"relevant_docs\": [11]},\n",
    "    #     {\"query\": \"health\", \"relevant_docs\": [14]},\n",
    "    #     {\"query\": \"deep learning AND large language models AND healthcare policies\", \"relevant_docs\": [0, 7, 14]},\n",
    "    #     {\"query\": \"artificial intelligence AND climate change AND medieval history\", \"relevant_docs\": [0, 12, 13]},\n",
    "    #     {\"query\": \"natural language processing AND cave exploration AND health\", \"relevant_docs\": [3, 16, 14]}\n",
    "    # ]\n",
    "\n",
    "    return test_queries\n",
    "\n",
    "# Evaluate for all algorithms\n",
    "def evaluate_system():\n",
    "    inverted_index = load_inverted_index()\n",
    "    articles = load_articles()\n",
    "\n",
    "    if not inverted_index or not articles:\n",
    "        print(\"Error loading data. Exiting...\")\n",
    "        return\n",
    "\n",
    "    test_queries = generate_test_queries()\n",
    "\n",
    "    algorithms = [\"boolean\", \"tfidf\", \"bm25\"]\n",
    "    for algorithm in algorithms:\n",
    "        print(f\"\\nEvaluating using {algorithm} search...\")\n",
    "        \n",
    "        precision_total = recall_total = f1_total = ap_total = 0\n",
    "        num_queries = len(test_queries)\n",
    "        \n",
    "        for query_data in test_queries:\n",
    "            query = query_data[\"query\"]\n",
    "            relevant_docs = query_data[\"relevant_docs\"]\n",
    "            \n",
    "            precision, recall, f1, ap = evaluate_search_algorithm(query, relevant_docs, inverted_index, articles, algorithm)\n",
    "            \n",
    "            precision_total += precision\n",
    "            recall_total += recall\n",
    "            f1_total += f1\n",
    "            ap_total += ap\n",
    "\n",
    "            print(f\"Query: '{query}' - Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}, MAP: {ap:.4f}\")\n",
    "        \n",
    "        print(f\"\\nAverage Precision: {precision_total / num_queries:.4f}\")\n",
    "        print(f\"Average Recall: {recall_total / num_queries:.4f}\")\n",
    "        print(f\"Average F1-Score: {f1_total / num_queries:.4f}\")\n",
    "        print(f\"Average MAP: {ap_total / num_queries:.4f}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    evaluate_system()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anakthsh_ergasia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
